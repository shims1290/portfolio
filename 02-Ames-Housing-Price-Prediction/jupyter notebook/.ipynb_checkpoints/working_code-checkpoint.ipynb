{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8fd083c",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Project 2 - Ames Housing Data and Kaggle Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0d4eb9",
   "metadata": {},
   "source": [
    "## Executive Summary\n",
    "---\n",
    "This project seeks to develop a housing sale price prediction model to help home sellers overcome imperfect information in the housing market, so that they can reliably predict housing prices and sell their houses at a fair value. \n",
    "\n",
    "Our attempt to develop a model yielded poor predictions on first try. This poor outcome is likely due to the highly restrictive feature selection process from the outset. We did not accomplish the intended goal of achieving predictions with root mean squared error lower than the cost of imperfect information in the housing market (estimated to be ~ \\$10,000 per sale). From this, we learn that there is a need to include more features (> 30) to train future models so that it can account for the wide variations sale prices in the unseen data. \n",
    "\n",
    "Beyond building a more complex model for housing sale price predictions, data scientist can also replicate the modelling process using housing sales dataset from other regions in the US (outside of Ames, Iowa) in future so that the model can be extended to predict housing prices in other geographical areas. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84660e82",
   "metadata": {},
   "source": [
    "## Background\n",
    "---\n",
    "\n",
    "With little understanding of the housing market, home sellers may encounter unfortunate events of mispricing of housing units at their point of sale.\n",
    "\n",
    "[Levitt and Syverson (2008)](https://ideas.repec.org/a/tpr/restat/v90y2008i4p599-611.html) flagged that **agents were often better informed than the clients who hire them**. Real estate agents may exploit this informational advantage and convince their clients to sell their houses too cheaply. Based on their research while controlling for observables, **they found that homes owned by real estate agents sold for 3.7% more than other houses**. This finding is worrying as it suggests that home sellers are likely to be disadvantaged when making a housing transaction with imperfect information.\n",
    "\n",
    "> According to the [Federal Reserve Bank of St. Louis](https://fred.stlouisfed.org/series/ASPUS), the average sales price of houses sold for the United States is **\\$278,000** in 2010 Q4. \n",
    "\n",
    "What this means is that imperfect information could cause home sellers to lose ~ **\\$10,000** (i.e. 3.7% * \\$278k) in housing value for the average transaction! \n",
    "\n",
    "Given there are **4.18 million** homes sold in the United States in 2010 [(source)](https://www.statista.com/statistics/226144/us-existing-home-sales/), the total cost of housing units mispricing (assuming 95% of these homes are not property agent owned) can come up to **\\\\$39 billion** per annum (i.e. 4.18million * \\$10k) ! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f82c99",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "---\n",
    "\n",
    "We want to help **uninformed home sellers** understand what constitute as fair housing prices by developing a regression model to **predict the sale prices of houses**. Specifically, we use linear models, i.e. ordinary least squares (OLS), Ridge and Lasso regressions. \n",
    "\n",
    "A successful housing price prediction model should be able to **predict housing prices with error term or root mean squared error that is ideally lower than $10,000** (i.e. the cost of imperfection information in the housing market)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aee188",
   "metadata": {},
   "source": [
    "## Data \n",
    "---\n",
    "\n",
    "We model and predict housing prices using the following datasets, drawn from housing sales in Ames, Iowa from 2006 to 2010:\n",
    "\n",
    "* `Train data:` [train.csv](./data/train.csv)\n",
    "* `Test data:` [test.csv](./data/test.csv)\n",
    "\n",
    "A data dictionary for the variables included in these datasets can be found [here](https://www.kaggle.com/c/dsi-us-11-project-2-regression-challenge/data). \n",
    "\n",
    "## Methodology\n",
    "---\n",
    "\n",
    "This project follows the following workflow:\n",
    "\n",
    "**1) Feature Selection**\n",
    "Apart from the housing sale prices, the train and test datasets contain 80 housing-related variables. We exercise judgement to include only select features that will most likely affect housing sale prices within our baseline model. This is to mitigate risks of overfitting and multicollinearity. We use pairplots to provide visualise and assess:\n",
    "\n",
    "- Whether variable has a linear relationship with target sale prices\n",
    "- The amount of variations within each variable \n",
    "- Possible collinearity and relationship between similar variables\n",
    "\n",
    "**2) Data Cleaning**\n",
    "We impute missing values and remove outliers.\n",
    "\n",
    "**3) Features Engineering**\n",
    "We dummify categorical variables (using drop_first), assign numerical rank values for ordinal variables, add interaction terms and engineer new features to strengthen the linear relationship of variable(s) with sale prices. \n",
    "\n",
    "**4) Model Preparation**\n",
    "We use the train-test split method to evaluate our model and scale our train (and test) data accordingly using StandardScaler.\n",
    "\n",
    "**5) Model Selection & Deployment**\n",
    "The best model among OLS, Ridge and Lasso regressions are selected using r2 scores metrics based on cross validation and initial model fitting between the train and test sets. \n",
    "\n",
    "The best model is then fitted and deployed to predict housing sale prices in the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc8be7d",
   "metadata": {},
   "source": [
    "## Data Import and Cleaning\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daadc762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import statistics\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a0be4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-67017f9d6295>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/test.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train.csv'"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c6ba48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"train:\", train.shape)\n",
    "print(\"test:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4b9ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standardizing the format of the column names \n",
    "train.columns = train.columns.str.lower().str.replace(' ', '_')\n",
    "test.columns = test.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7652949",
   "metadata": {},
   "source": [
    "### Housing prices are likely associated with the size of living area. \n",
    "### We scatter `saleprice` against `gr_liv_area` (i.e. above grade (ground) living area square feet) in square feet to check for outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e11562",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.title(\"Scatterplot of sale price and size of living area in train data\", fontsize=20)\n",
    "plt.ylabel(\"housing sale price ($)\", fontsize=15)\n",
    "plt.xlabel(\"ground living area (sqft)\", fontsize=15)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.axvline(4000, color=\"r\")\n",
    "plt.scatter(train[\"gr_liv_area\"], train[\"saleprice\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47bd8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "plt.title(\"Distribution of size of ground living area in train data\", fontsize=20)\n",
    "plt.ylabel(\"frequency\", fontsize=15)\n",
    "plt.xlabel(\"ground living area (sqft)\", fontsize=15)\n",
    "plt.hist(train[\"gr_liv_area\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00acd6e7",
   "metadata": {},
   "source": [
    "### Majority of the housing units have ground living area lower than 4,000 square feet. Therefore, it would not cause too much skewing of actual train data if we drop the 2 outliers above (i.e. houses with >4,000 sqft living area being sold for < $200,000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73597ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train.loc[train[\"gr_liv_area\"] < 4000]\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2494239",
   "metadata": {},
   "source": [
    "### We plot a series of histogram plots to \n",
    "* ### (i) eyeball the distribution of the numerical variables; and, \n",
    "* ### (ii) spot data entry error(s) within the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d4f9ea",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.hist(figsize=(20, 25));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e2765a",
   "metadata": {},
   "source": [
    "### We remove the row with data entry error under *garage_yr_blt* below as it cannot possibly be built after 2010. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12c592d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.drop(train[train[\"garage_yr_blt\"] > 2011].index, inplace=True)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a499e",
   "metadata": {},
   "source": [
    "### After removing the outliers and error, we proceed to drop variables with no clear linear relationship with sale price (see pairplots).\n",
    "\n",
    "### However, we will not include all variables showing linear relationships with sale price as it will risk multicollinearity.\n",
    "\n",
    "> E.g. `*garage_cars*` and `*garage_area*` are likely to be highly related. \n",
    "\n",
    "\\* For first cut, variables may be dropped based on subjective assessment. We can always add the variables back later if there is an issue of under-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09302b62",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.pairplot(train, y_vars=['id', 'pid', 'ms_subclass', 'ms_zoning', 'lot_frontage', 'lot_area',\n",
    "       'street', 'alley', 'lot_shape', 'land_contour', 'utilities',\n",
    "       'lot_config', 'land_slope', 'neighborhood', 'condition_1',\n",
    "       'condition_2', 'bldg_type', 'house_style', 'overall_qual',\n",
    "       'overall_cond', 'year_built', 'year_remod/add', 'roof_style',\n",
    "       'roof_matl', 'exterior_1st', 'exterior_2nd', 'mas_vnr_type',\n",
    "       'mas_vnr_area', 'exter_qual', 'exter_cond', 'foundation', 'bsmt_qual',\n",
    "       'bsmt_cond', 'bsmt_exposure', 'bsmtfin_type_1', 'bsmtfin_sf_1',\n",
    "       'bsmtfin_type_2', 'bsmtfin_sf_2', 'bsmt_unf_sf', 'total_bsmt_sf',\n",
    "       'heating', 'heating_qc', 'central_air', 'electrical', '1st_flr_sf',\n",
    "       '2nd_flr_sf', 'low_qual_fin_sf', 'gr_liv_area', 'bsmt_full_bath',\n",
    "       'bsmt_half_bath', 'full_bath', 'half_bath', 'bedroom_abvgr',\n",
    "       'kitchen_abvgr', 'kitchen_qual', 'totrms_abvgrd', 'functional',\n",
    "       'fireplaces', 'fireplace_qu', 'garage_type', 'garage_yr_blt',\n",
    "       'garage_finish', 'garage_cars', 'garage_area', 'garage_qual',\n",
    "       'garage_cond', 'paved_drive', 'wood_deck_sf', 'open_porch_sf',\n",
    "       'enclosed_porch', '3ssn_porch', 'screen_porch', 'pool_area', 'pool_qc',\n",
    "       'fence', 'misc_feature', 'misc_val', 'mo_sold', 'yr_sold', 'sale_type'], \n",
    "             x_vars=['saleprice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7777b35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([\"id\", \"pid\",\"ms_subclass\", \"ms_zoning\", \"street\", \"alley\", \"lot_shape\", \"utilities\", \"land_contour\",\n",
    "           \"land_slope\", \"condition_1\", \"condition_2\", \"bldg_type\", \"house_style\", \"roof_style\", \"pool_qc\",\n",
    "           \"roof_matl\", \"exterior_1st\", \"exterior_2nd\", \"mas_vnr_type\", \"mas_vnr_area\", \"foundation\", \"year_built\",\n",
    "           \"bsmt_exposure\", \"bsmtfin_type_1\", \"bsmtfin_sf_1\", \"bsmtfin_type_2\", \"bsmtfin_sf_2\", \"bsmt_unf_sf\",\n",
    "           \"heating\", \"electrical\", \"1st_flr_sf\", \"2nd_flr_sf\", \"low_qual_fin_sf\", \"totrms_abvgrd\", \n",
    "           \"garage_type\", \"garage_yr_blt\", \"garage_finish\", \"garage_cars\", \"paved_drive\", \"fence\",  \n",
    "            \"misc_feature\", \"misc_val\", \"mo_sold\", \"sale_type\"], \n",
    "           axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3600f02",
   "metadata": {},
   "source": [
    "### We impute missing values within the train set below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292e867b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae575d",
   "metadata": {},
   "source": [
    "### Lot frontage may be related with different lot configuration. \n",
    "\n",
    "We impute the missing values for `lot_frontage` according to the mean of each `lot_config`. This would likely be more accurate then imputing the missing values based on the overall mean of the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dbd08f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.groupby(\"lot_config\")[\"lot_frontage\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f20da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"lot_frontage_imputed\"] = train[\"lot_frontage\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1dc876",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lot_config = [\"Corner\", \"CulDSac\", \"FR2\", \"FR3\", \"Inside\"]\n",
    "\n",
    "for i in lot_config:\n",
    "    train.loc[(train[\"lot_frontage\"].isna()) & (train[\"lot_config\"]==i), \n",
    "              \"lot_frontage_imputed\"] = train.groupby(\"lot_config\")[\"lot_frontage\"].mean()[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd47e5bb",
   "metadata": {},
   "source": [
    "After imputing the missing values, we drop the `lot_frontage` & `lot_config` as they are no longer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f76706",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([\"lot_frontage\", \"lot_config\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15652a01",
   "metadata": {},
   "source": [
    "### We further impute missing values for the other variables below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573efb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### e.g. NA quality for fireplace=0\n",
    "\n",
    "missing_var = [\"fireplace_qu\", \"bsmt_qual\", \"bsmt_full_bath\", \"bsmt_half_bath\", \"garage_area\",\n",
    "              \"garage_qual\", \"total_bsmt_sf\", \"garage_cond\", \"bsmt_cond\", \"exter_cond\", \"pool_area\"]\n",
    "\n",
    "for i in missing_var:\n",
    "    if train[i].dtype == \"O\":\n",
    "        train[i].fillna(\"NA\", inplace=True)\n",
    "    else:\n",
    "        train[i].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75db4f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9908706",
   "metadata": {},
   "source": [
    "### Now that there are no more missing values, we proceed to engineer new features for our regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc74563b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature engineering: aggregate similar or related variables\n",
    "\n",
    "train[\"age\"] = train[\"yr_sold\"] - train[\"year_remod/add\"]\n",
    "train[\"porch_area\"] = train[\"open_porch_sf\"] + train[\"enclosed_porch\"] + train[\"3ssn_porch\"] + train[\"screen_porch\"]\n",
    "train[\"total_rooms\"] = train[\"full_bath\"] + train[\"half_bath\"] +train[\"bedroom_abvgr\"] + train[\"kitchen_abvgr\"] + train[\"bsmt_full_bath\"] + train[\"bsmt_half_bath\"]\n",
    "\n",
    "train.drop([\"year_remod/add\", \"open_porch_sf\", \"enclosed_porch\", \"3ssn_porch\", \"screen_porch\",\n",
    "            \"full_bath\", \"half_bath\", \"bedroom_abvgr\", \"kitchen_abvgr\", \"bsmt_full_bath\", \"bsmt_half_bath\"], \n",
    "           axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aefdf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature engineering: change all the ordinal variables into numerical variables \n",
    "ordinal_var = [\"bsmt_qual\", \"heating_qc\", \"kitchen_qual\", \"fireplace_qu\", \"garage_qual\", \"exter_qual\",\n",
    "              \"exter_cond\", \"bsmt_cond\", \"garage_cond\"]\n",
    "\n",
    "for i in ordinal_var:\n",
    "    train[i].replace({\"Ex\": 5, \"Gd\":4, \"TA\": 3, \"Fa\": 2, \"Po\":1, \"NA\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdba3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Feature engineering: come up with neighbourhood score based on:\n",
    "# (i) % of housing units in the neighbourhood with typical functionality\n",
    "# (ii) score of the overall quality and condition of housing units within each neighbourhood\n",
    "# (iii) score of the exterior quality and condition of housing units within each neighbourhood\n",
    "\n",
    "train[\"typical_functionality\"] = 0\n",
    "train.loc[(train[\"functional\"] == \"Typ\"), \"typical_functionality\"] = 1\n",
    "train.drop(\"functional\", axis=1, inplace=True)\n",
    "\n",
    "# Create interaction variables between overall_qual & overall_cond AND exter_qual & exter_cond\n",
    "train[\"overall_qual*cond\"] = train[\"overall_qual\"] * train[\"overall_cond\"]\n",
    "train[\"exter_qual*cond\"] = train[\"exter_qual\"] * train[\"exter_cond\"]\n",
    "\n",
    "neighborhd = train.groupby(\"neighborhood\", as_index=False)[[\"typical_functionality\", \"overall_qual*cond\", \"exter_qual*cond\" ]].mean()\n",
    "neighborhd[\"tf_rank\"] = neighborhd[\"typical_functionality\"].rank()\n",
    "neighborhd[\"overall_rank\"] = neighborhd[\"overall_qual*cond\"].rank()\n",
    "neighborhd[\"exter_rank\"] = neighborhd[\"exter_qual*cond\"].rank()\n",
    "neighborhd[\"composite_rank_score\"] = neighborhd[\"tf_rank\"] + neighborhd[\"overall_rank\"] + neighborhd[\"exter_rank\"]\n",
    "neighborhd[\"rank_order\"] = neighborhd[\"composite_rank_score\"].rank()\n",
    "\n",
    "neighborhd[\"neighborhood_score\"] = 0\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 4), \"neighborhood_score\"] = 1\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 8) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 2\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 12) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 3\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 16) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 4\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 20) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 5\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 24) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 6\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 28) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 7\n",
    "\n",
    "train = train.merge(neighborhd[[\"neighborhood\", \"neighborhood_score\"]], how=\"inner\", on=\"neighborhood\")\n",
    "train.drop(\"neighborhood\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66703e5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,5))\n",
    "plt.title(\"Scatterplot for housing sale price vs the neighborhood score\", fontsize=18)\n",
    "plt.ylim(0, 7.5)\n",
    "plt.xlim(0, 600000)\n",
    "sns.regplot(x=\"saleprice\", y=\"neighborhood_score\", data=train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cb3c3b",
   "metadata": {},
   "source": [
    "The above scatterplot shows that the engineered neighborhood score feature as a linear relationship to the target variable (i.e. saleprices)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca64581",
   "metadata": {},
   "source": [
    "### Finally, we dummify the remaining categorical variables within the dataset, i.e. using one-hot code method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d7f514",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"central_air\", \"yr_sold\"]\n",
    "\n",
    "for i in cols:\n",
    "    one_hot = pd.get_dummies(train[i], prefix=i, drop_first=True) \n",
    "    train = train.drop(i,axis = 1)\n",
    "    train = train.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c37b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Move saleprice to final col\n",
    "cols = train.columns.tolist()\n",
    "cols = cols[0:18] + cols[19::] + cols[18:19] \n",
    "train = train[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f93e72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce967e9",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d8e7f2",
   "metadata": {},
   "source": [
    "### The distribution of housing sale price seems positively skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f5cd34",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Distribution of housing sale price in the train dataset\", fontsize=20)\n",
    "plt.hist(train[\"saleprice\"])\n",
    "plt.axvline(train[\"saleprice\"].mean(), color=\"red\")\n",
    "print(f'The mean housing sale price is $',round(train[\"saleprice\"].mean(),2),' (see red line below).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2c0882",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.title(\"Boxplot of housing sale price in the train dataset\", fontsize=20)\n",
    "sns.boxplot(train[\"saleprice\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b842705",
   "metadata": {},
   "source": [
    "There is a wide spread of housing prices with a significant number of outliers. \n",
    "\n",
    "The interquartile range is from \\\\$ 129725.00 to \\$ 214000.00. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea63db97",
   "metadata": {},
   "source": [
    "### The variables most highly correlated with sale prices are:\n",
    "- `overall_qual`\n",
    "- `gr_liv_area`\n",
    "- `exter_qual`\n",
    "- `kitchen_qual`\n",
    "- `neighborhood_score`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e166f780",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train.corr()[\"saleprice\"].sort_values(ascending=False)[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333d07c2",
   "metadata": {},
   "source": [
    "### The summary statistics of the numerical, non-ordinal variables most correlated with sale prices are presented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e8da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train[[\"saleprice\", \"gr_liv_area\", \"total_bsmt_sf\", \"garage_area\"]].describe().round(2).iloc[1::,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20af456d",
   "metadata": {},
   "source": [
    "A quick look at the numbers above tells us that sale prices share a positive relationship with the size of living area, basement area and garage area. However, the sale price is not exactly directly proportional to the size of these areas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536ecd5",
   "metadata": {},
   "source": [
    "### For the strongest predictor, `overall_qual`, we take a further look at its distribution and boxplot below.\n",
    "\n",
    "### Similar to the distribution of housing sale prices, the distribution of overall quality scores is also positively skewed but less so. This suggests there are other important variables to account for the variability in housing sale prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c5f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,8))\n",
    "plt.title(\"Distribution of overall quality in the train dataset\", fontsize=20)\n",
    "plt.hist(train[\"overall_qual\"])\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.axvline(train[\"overall_qual\"].mean(), color=\"red\")\n",
    "print(f'The mean overall quality is',round(train[\"overall_qual\"].mean(),2),' out of 10 (see red line below).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a88747",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,5))\n",
    "plt.title(\"Boxplot of overall quality score in the train dataset\", fontsize=20)\n",
    "sns.boxplot(train[\"overall_qual\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241d33f9",
   "metadata": {},
   "source": [
    "There is a wide range of overall quality scores observed across all housing units. Half of them score between 5 to 7 points (out of 10). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc944bf",
   "metadata": {},
   "source": [
    "### Now we plot a heatmap to look at the correlation matrix of the variables within the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef439f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the Upper Triangle of the co-relation matrix\n",
    "matrix = np.triu(train.corr())\n",
    "\n",
    "# Using the upper triangle matrix as mask \n",
    "plt.figure(figsize=(20,25))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(train.corr(), annot=True, mask= matrix, cmap=\"coolwarm\", \n",
    "           fmt=\".2f\", annot_kws={\"size\":12}, vmin=-1, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45173797",
   "metadata": {},
   "source": [
    "Focussing on the final row of the heatmap, ~1/3 of the chosen variables have moderately strong correlation with housing sale prices (r > 0.5), so it is likely that this set of variables will be able to help with the prediction of housing sale prices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f8e11c",
   "metadata": {},
   "source": [
    "### We now focus on the dependent variables that are strongly correlated (r > 0.8) to avoid multicollinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d32476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the upper triangle matrix as mask \n",
    "plt.figure(figsize=(20,25))\n",
    "sns.set(font_scale=1.4)\n",
    "sns.heatmap(train.corr(), annot=True, mask= matrix | (np.abs(train.corr()) < 0.8), cmap=\"coolwarm\", \n",
    "           fmt=\".2f\", linewidths=0.5, linecolor='grey', annot_kws={\"size\":12}, vmin=-1, vmax=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb944c7",
   "metadata": {},
   "source": [
    "### Based on the heatmap above, we observe that `exter_qual*cond` `fireplaces` and `garage_cond` are highly related to `exter_qual`, `fireplace_qu` and `garage_qual` respectively. \n",
    "\n",
    "We drop `exter_qual*cond`, `fireplaces` and `garage_cond` from our train dataset as they are less correlated with the housing sale prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c072a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([\"exter_qual*cond\", \"fireplaces\", \"garage_cond\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b9e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25072841",
   "metadata": {},
   "source": [
    "## Model Prep\n",
    "---\n",
    "### Create our features matrix (`X`) and target vector (`y`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99798b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.iloc[:,0:-1]\n",
    "y = train[\"saleprice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a39ff5",
   "metadata": {},
   "source": [
    "### Train/test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d81689",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec570f",
   "metadata": {},
   "source": [
    "### Scaling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e36673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train = ss.transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3229192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'X_train shape is: {X_train.shape}')\n",
    "print(f'y_train shape is: {y_train.shape}')\n",
    "print(f'X_test shape is: {X_test.shape}')\n",
    "print(f'y_test shape is: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4495b13b",
   "metadata": {},
   "source": [
    "### Instantiate the 3 models: OLS, Ridge, Lasso and fit the models to get preliminary R2 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54b613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols = LinearRegression()\n",
    "ols.fit(X_train, y_train)\n",
    "print(\"The ols R2 score for the train set is:\", ols.score(X_train, y_train))\n",
    "print(\"The ols R2 score for the test set is:\", ols.score(X_test, y_test))\n",
    "print(\"The difference between above scores is: \", round(ols.score(X_train, y_train) - ols.score(X_test, y_test), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9741f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV(alphas=np.logspace(0, 5, 100), scoring='r2', cv=5)\n",
    "ridge.fit(X_train, y_train)\n",
    "print(\"The ridge R2 score for the train set is:\", ridge.score(X_train, y_train))\n",
    "print(\"The ridge R2 score for the test set is:\", ridge.score(X_test, y_test))\n",
    "print(\"The difference between above scores is: \", round(ridge.score(X_train, y_train) - ridge.score(X_test, y_test), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad81529",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV(alphas=np.logspace(-3, 0, 100), cv=5)\n",
    "lasso.fit(X_train, y_train)\n",
    "print(\"The lasso R2 score for the train set is:\", lasso.score(X_train, y_train))\n",
    "print(\"The lasso R2 score for the test set is:\", lasso.score(X_test, y_test))\n",
    "print(\"The difference between above scores is: \", round(lasso.score(X_train, y_train) - lasso.score(X_test, y_test), 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd8a8de",
   "metadata": {},
   "source": [
    "Typically, we choose the model that gives us the highest R2 scores for both train and test set as that would mean that the model is able to explain more variabilities observed in the depedent variable. At the same time, we also care about the difference between the R2 scores of the train and test data. A greater difference between both scores suggests that there may be a risk of overfitting to the train dataset and the model may not work as well on unseen data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f6e3c",
   "metadata": {},
   "source": [
    "### As the difference between the r2 scores of the train and test sets are not large across all three models, it is likely that any given model will generalise to new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d32525",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_scores = cross_val_score(ols, X_train, y_train, cv=5)\n",
    "ols_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec28edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_scores = cross_val_score(ridge, X_train, y_train, cv=5)\n",
    "ridge_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47020f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_scores = cross_val_score(lasso, X_train, y_train, cv=5)\n",
    "lasso_scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994da2aa",
   "metadata": {},
   "source": [
    "### Based on the cross validation (cv) above, the ridge model looks the most promising as it has the highest mean cv r2 score. Therefore, we proceed with it for subsequent fitting with the entire train data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532b7ed4",
   "metadata": {},
   "source": [
    "### Scaling, Model Fitting and Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd383a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = ss.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e05e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ridge.fit(X_scaled, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0aa58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.score(X_scaled, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84766aa8",
   "metadata": {},
   "source": [
    "### We plot a bar chart to look at the size of coefficients of the respective predictors in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e2d840",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.title(\"Coefficients in the ridge regession model\")\n",
    "pd.Series(ridge.coef_, index=X.columns).sort_values(ascending=False).plot.bar(figsize=(15, 7));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28b8563",
   "metadata": {},
   "source": [
    "Based on the above chart, we can infer the following:\n",
    "- The size of ground living area, total basement area in square ft, garage area, lot area and etc. affects housing sale price significantly (the larger the areas, the higher the sale price).  \n",
    "\n",
    "\n",
    "- The overall and external quality of the housing units are also key in influencing the housing sale prices (the higher the quality, the higher the sale price). If home sellers would like to increase the value of their houses at point of sale, they should take note to improve the overall, external and kitchen quality. \n",
    "\n",
    "\n",
    "- If a housing unit is based in a neighbourhood that is associated with more housing units having positive features (e.g. typical functionality, high overall and external quality and condition), the sale prices tend to be higher. This means that sellers putting up their homes in good neighborhood should not just price their houses for sale based on their unit-specific characteristics.  \n",
    "\n",
    "\n",
    "- For dummy variables, we need to infer the coefficients relative to the dummy variable that was dropped. For example, for the dummy variable `central_air_Y`, which indicates 1 if the housing unit has central air conditioning, we interpret it's coefficient as the difference in housing sale price as compared to a housing unit without central air conditioning. In this case, houses with central air conditioning has a small premium over houses without given then small, positive coefficient value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8990cae",
   "metadata": {},
   "source": [
    "## Further model evaluation (against null or baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e77bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = ridge.predict(X_scaled)\n",
    "residuals = y - pred\n",
    "mse = np.mean(residuals**2)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009e4f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bar = np.mean(y)\n",
    "null_mse = np.mean((y - y_bar)**2)\n",
    "null_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ab3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse < null_mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b41d9",
   "metadata": {},
   "source": [
    "Mean squared error (MSE) is a useful scoring metric. The lower the score, the better the model performance. A model will get a lower score when the predictions generated are closer to the true values. As the MSE of the ridge model is lower than the MSE of the null/baseline model (involving the use of y_bar or mean y value as prediction values), our model is good for deployment and further testing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5bac85",
   "metadata": {},
   "source": [
    "### The errors of the model seem normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92daec0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"The distribution of size of errors across the model\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"size of error; also known as the residual\")\n",
    "plt.hist(residuals, bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f824a2",
   "metadata": {},
   "source": [
    "### However, we might have issues with further model deployment as the variance for the errors is not equal across all predicted values, i.e. there is some heteroscedasticity implied by the plot below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989421c7",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.title(\"Scatterplot of residual against predicted housing sale price\", fontsize=20)\n",
    "plt.scatter(pred, residuals)\n",
    "plt.axhline(0, color=\"orange\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe01a3d",
   "metadata": {},
   "source": [
    "Regardless, we can proceed to clean the test data so that we can deploy the fitted model and test the prediction against the unseen data..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c620c",
   "metadata": {},
   "source": [
    "## Replicating the data cleaning steps for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f8d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\")\n",
    "test.columns = test.columns.str.lower().str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b116a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop([\"id\", \"pid\",\"ms_subclass\", \"ms_zoning\", \"street\", \"alley\", \"lot_shape\", \"utilities\", \"land_contour\",\n",
    "           \"land_slope\", \"condition_1\", \"condition_2\", \"bldg_type\", \"house_style\", \"roof_style\", \"pool_qc\",\n",
    "           \"roof_matl\", \"exterior_1st\", \"exterior_2nd\", \"mas_vnr_type\", \"mas_vnr_area\", \"foundation\", \"year_built\",\n",
    "           \"bsmt_exposure\", \"bsmtfin_type_1\", \"bsmtfin_sf_1\", \"bsmtfin_type_2\", \"bsmtfin_sf_2\", \"bsmt_unf_sf\",\n",
    "           \"heating\", \"electrical\", \"1st_flr_sf\", \"2nd_flr_sf\", \"low_qual_fin_sf\", \"totrms_abvgrd\", \n",
    "           \"garage_type\", \"garage_yr_blt\", \"garage_finish\", \"garage_cars\", \"paved_drive\", \"fence\",  \n",
    "            \"misc_feature\", \"misc_val\", \"mo_sold\", \"sale_type\"], \n",
    "           axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf68811",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.groupby(\"lot_config\")[\"lot_frontage\"].mean()\n",
    "test[\"lot_frontage_imputed\"] = test[\"lot_frontage\"]\n",
    "\n",
    "lot_config = [\"Corner\", \"CulDSac\", \"FR2\", \"FR3\", \"Inside\"]\n",
    "for i in lot_config:\n",
    "    test.loc[(test[\"lot_frontage\"].isna()) & (test[\"lot_config\"]==i), \"lot_frontage_imputed\"] \\\n",
    "    = test.groupby(\"lot_config\")[\"lot_frontage\"].mean()[i]\n",
    "\n",
    "test.drop([\"lot_frontage\", \"lot_config\"], inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b5b54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_var = [\"fireplace_qu\", \"bsmt_qual\", \"bsmt_full_bath\", \"bsmt_half_bath\", \"garage_area\",\n",
    "              \"garage_qual\", \"total_bsmt_sf\", \"garage_cond\", \"bsmt_cond\", \"exter_cond\", \"pool_area\"]\n",
    "\n",
    "for i in missing_var:\n",
    "    if test[i].dtype == \"O\":\n",
    "        test[i].fillna(\"NA\", inplace=True)\n",
    "    else:\n",
    "        test[i].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a0460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"age\"] = test[\"yr_sold\"] - test[\"year_remod/add\"]\n",
    "test[\"porch_area\"] = test[\"open_porch_sf\"] + test[\"enclosed_porch\"] + test[\"3ssn_porch\"] + test[\"screen_porch\"]\n",
    "test[\"total_rooms\"] = test[\"full_bath\"] + test[\"half_bath\"] +test[\"bedroom_abvgr\"] + test[\"kitchen_abvgr\"] + test[\"bsmt_full_bath\"] + test[\"bsmt_half_bath\"]\n",
    "\n",
    "test.drop([\"year_remod/add\", \"open_porch_sf\", \"enclosed_porch\", \"3ssn_porch\", \"screen_porch\",\n",
    "            \"full_bath\", \"half_bath\", \"bedroom_abvgr\", \"kitchen_abvgr\", \"bsmt_full_bath\", \"bsmt_half_bath\"], \n",
    "           axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf678767",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_var = [\"bsmt_qual\", \"heating_qc\", \"kitchen_qual\", \"fireplace_qu\", \"garage_qual\", \"exter_qual\",\n",
    "              \"exter_cond\", \"bsmt_cond\", \"garage_cond\"]\n",
    "\n",
    "for i in ordinal_var:\n",
    "    test[i].replace({\"Ex\": 5, \"Gd\":4, \"TA\": 3, \"Fa\": 2, \"Po\":1, \"NA\": 0}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148ffc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"typical_functionality\"] = 0\n",
    "test.loc[(test[\"functional\"] == \"Typ\"), \"typical_functionality\"] = 1\n",
    "test.drop(\"functional\", axis=1, inplace=True)\n",
    "\n",
    "test[\"overall_qual*cond\"] = test[\"overall_qual\"] * test[\"overall_cond\"]\n",
    "test[\"exter_qual*cond\"] = test[\"exter_qual\"] * test[\"exter_cond\"]\n",
    "\n",
    "neighborhd = test.groupby(\"neighborhood\", as_index=False)[[\"typical_functionality\", \"overall_qual*cond\", \"exter_qual*cond\"]].mean()\n",
    "neighborhd[\"tf_rank\"] = neighborhd[\"typical_functionality\"].rank()\n",
    "neighborhd[\"overall_rank\"] = neighborhd[\"overall_qual*cond\"].rank()\n",
    "neighborhd[\"exter_rank\"] = neighborhd[\"exter_qual*cond\"].rank()\n",
    "neighborhd[\"composite_rank_score\"] = neighborhd[\"tf_rank\"] + neighborhd[\"overall_rank\"] + neighborhd[\"exter_rank\"]\n",
    "neighborhd[\"rank_order\"] = neighborhd[\"composite_rank_score\"].rank()\n",
    "\n",
    "neighborhd[\"neighborhood_score\"] = 0\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 4), \"neighborhood_score\"] = 1\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 8) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 2\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 12) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 3\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 16) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 4\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 20) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 5\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 24) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 6\n",
    "neighborhd.loc[(neighborhd[\"rank_order\"] <= 28) & (neighborhd[\"neighborhood_score\"]==0), \"neighborhood_score\"] = 7\n",
    "\n",
    "test = test.merge(neighborhd[[\"neighborhood\", \"neighborhood_score\"]], how=\"inner\", on=\"neighborhood\")\n",
    "test.drop(\"neighborhood\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8adcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"central_air\", \"yr_sold\"]\n",
    "\n",
    "for i in cols:\n",
    "    one_hot = pd.get_dummies(test[i], prefix=i, drop_first=True) \n",
    "    test = test.drop(i,axis = 1)\n",
    "    test = test.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589fd559",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.drop([\"exter_qual*cond\", \"fireplaces\", \"garage_cond\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598d74a1",
   "metadata": {},
   "source": [
    "## Predicting the housing sale prices for the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955ab4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scaled = ss.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8a907f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'saleprice':ridge.predict(test_scaled)})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d3ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_id = pd.read_csv(\"data/test.csv\")\n",
    "kaggle_id = kaggle_id.iloc[:,0:1]\n",
    "kaggle_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a92d41e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "kaggle_sub = pd.concat([kaggle_id, df.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_sub.to_csv(\"kaggle_submission_dsi23_lee_shi_min.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835b37a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "---\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "After submitting the predicted housing sale prices to kaggle, we obtained a **very high RMSE score of more than 100,000**. This suggests that our model is **not** generalizable to unseen data. As the RMSE is much higher than \\$10,000 (i.e. the estimated cost of imperfect information in the housing market - see background), it is **unlikely that home sellers will benefit from the use of this sale price prediction model**. \n",
    "\n",
    "Nevertheless, we learn from this modelling exercise that the quality of the housing aspects are much more important than its corresponding condition. Home sellers hoping to raise the value of their future housing sales should look into improving the overall, external quality of their homes. \n",
    "\n",
    "This failure of this data science project could be due to the highly restrictive feature selection process from the outset. As there is a large spread and types of housing units included in the training and test datasets, inclusion of more features ( > 30) will be required to train the model and account for more variations in sale prices of the unseen data. \n",
    "\n",
    "**Recommendation:**\n",
    "\n",
    "We recommend for a ***new*** model to be developed to improve housing sale price prediction with at least 30 housing features incorporated as the existing model is too weak in terms of sale price prediction to be useful for our stakeholders. Perhaps, a better way to approach feature selection in this context is to include all variables from the outset and apply lasso regression to regularize the model. \n",
    "\n",
    "**Future Scope:**\n",
    "\n",
    "Beyond building a more complex model for housing sale price predictions, data scientist can also replicate the modelling process using housing sales dataset from other regions in the US (outside of Ames, Iowa) in future so that the model can be extended to predict housing prices in other geographical areas. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
